{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook shows how to use ZKLoRA to prove the forward pass of a LoRA model.\n",
    "\n",
    "# Setup\n",
    "\n",
    "In order to use this notebook, first install the zklora package:\n",
    "\n",
    "```bash\n",
    "pip install zklora\n",
    "```\n",
    "\n",
    "Or, if you are running this notebook locally and from the `examples` directory in this repository, you have to let the notebook know where to find the zklora package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can import the necessary functions from the zklora package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zklora import export_lora_submodules, generate_proofs, batch_verify_proofs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also use the Hugging Face Transformers and Peft libraries to load the model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `distilgpt2` model as the base model and the `q1e123/peft-starcoder-lora-a100` model as the LoRA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = \"distilgpt2\"\n",
    "lora_model_name = \"q1e123/peft-starcoder-lora-a100\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "lora_model = PeftModel.from_pretrained(base_model, lora_model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "lora_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activations from the Base Model\n",
    "\n",
    "In order to prove the forward pass of the LoRA model, we need to export the activations from the base model.\n",
    "\n",
    "To that end, we call the `export_lora_submodules` function. The `export_lora_submodules` function will export the activations from the base model for the given input texts and submodule key.\n",
    "\n",
    "An input text is required for the base model to generate the activations. The activations are exported to the `intermediate_activations` directory as `json` files.\n",
    "\n",
    "In the directory `lora_onnx_params` we will find the ONNX files for the LoRA model. The input text is also required to generate the ONNX files.\n",
    "\n",
    "With the `submodule_key` we specify the name of the submodule we want to export. In this case, we want to export the activations from the `attn.c_attn` submodule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"Hello from LoRA\", \"And another test\", \"One more line...\"]\n",
    "\n",
    "export_lora_submodules(\n",
    "    model=lora_model,\n",
    "    tokenizer=tokenizer,\n",
    "    input_texts=texts,\n",
    "    submodule_key=\"attn.c_attn\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calling the `export_lora_submodules` function, we will see two directories created: `intermediate_activations` and `lora_onnx_params`.\n",
    "\n",
    "The `intermediate_activations` directory will contain the activations from the base model for the given input texts.\n",
    "\n",
    "The `lora_onnx_params` directory will contain the ONNX files for the LoRA model.\n",
    "\n",
    "The `export_lora_submodules` function is asynchronous, so it will return immediately and the activations will be generated in the background.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Proofs\n",
    "\n",
    "Now we can generate the proofs. For that, we call the `generate_proofs` function. This function will generate a proof for each pair of ONNX and JSON files in the `lora_onnx_params` and `intermediate_activations` directories.\n",
    "\n",
    "The `generate_proofs` function is asynchronous, so it will return immediately and the proofs will be generated in the background. However, to make this work, and since we are using a notebook, you'll need to run the cell with the await statement in an async context.\n",
    "\n",
    "Since the generation of proofs is a time-consuming process, it will take around 10 minutes depending on the hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    return await generate_proofs(verbose=True)\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `generate_proofs` function will create a directory called `proof_artifacts` in the current working directory. This directory will contain the proof artifacts for each pair of ONNX and JSON files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verifying Proofs\n",
    "\n",
    "Now we can verify the proofs. For that, we call the `batch_verify_proofs` function. This function will verify the proofs generated that are stored in the `proof_artifacts` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_verify_proofs(verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
