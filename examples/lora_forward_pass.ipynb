{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook shows how to use ZKLoRA to prove the forward pass of a LoRA model.\n",
    "\n",
    "# Setup\n",
    "\n",
    "In order to use this notebook, first install the zklora package:\n",
    "\n",
    "```bash\n",
    "pip install zklora\n",
    "```\n",
    "\n",
    "Or, if you are running this notebook locally and from the `examples` directory in this repository, you have to let the notebook know where to find the zklora package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can import the necessary functions from the zklora package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zklora import export_lora_submodules, generate_proofs, batch_verify_proofs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also use the Hugging Face Transformers and Peft libraries to load the model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPT2LMHeadModel(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(50257, 768)\n",
       "        (wpe): Embedding(1024, 768)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-5): 6 x GPT2Block(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2SdpaAttention(\n",
       "              (c_attn): lora.Linear(\n",
       "                (base_layer): Conv1D(nf=2304, nx=768)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (c_proj): lora.Linear(\n",
       "                (base_layer): Conv1D(nf=768, nx=768)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): lora.Linear(\n",
       "                (base_layer): Conv1D(nf=3072, nx=768)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (c_proj): lora.Linear(\n",
       "                (base_layer): Conv1D(nf=768, nx=3072)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model_name = \"distilgpt2\"\n",
    "lora_model_name = \"q1e123/peft-starcoder-lora-a100\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "lora_model = PeftModel.from_pretrained(base_model, lora_model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "lora_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"Hello from LoRA\", \"And another test\", \"One more line...\"]\n",
    "\n",
    "export_lora_submodules(\n",
    "    model=lora_model,\n",
    "    tokenizer=tokenizer,\n",
    "    input_texts=texts,\n",
    "    submodule_key=\"attn.c_attn\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takes around 10 minutes depending on the hardware.\n",
    "\n",
    "To make this work, you'll need to run the cell with the await statement in an async context. \n",
    "\n",
    "wrap your code in an async function and use IPython's async integration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 ONNX files in lora_onnx_params.\n",
      "Processing ONNX files for proof generation...\n",
      "==========================================\n",
      "Preparing to prove with ONNX: lora_onnx_params/base_model_model_transformer_h_5_attn_c_attn.onnx\n",
      "Matching JSON: intermediate_activations/base_model_model_transformer_h_5_attn_c_attn.json\n",
      "Number of parameters: 24,576\n",
      "Generating settings & compiling circuit...\n",
      "Setup for base_model_model_transformer_h_5_attn_c_attn took 78.48 sec\n",
      "Input shape from JSON: (1, 9216)\n",
      "Local ONNX output shape: (1, 27648)\n",
      "Generating witness (async)...\n",
      "Witness gen took 12.75 sec\n",
      "Generating proof...\n",
      "Proof gen took 38.53 sec\n",
      "Done with base_model_model_transformer_h_5_attn_c_attn.\n",
      "\n",
      "==========================================\n",
      "Preparing to prove with ONNX: lora_onnx_params/base_model_model_transformer_h_1_attn_c_attn.onnx\n",
      "Matching JSON: intermediate_activations/base_model_model_transformer_h_1_attn_c_attn.json\n",
      "Number of parameters: 24,576\n",
      "Generating settings & compiling circuit...\n",
      "Setup for base_model_model_transformer_h_1_attn_c_attn took 44.72 sec\n",
      "Input shape from JSON: (1, 9216)\n",
      "Local ONNX output shape: (1, 27648)\n",
      "Generating witness (async)...\n",
      "Witness gen took 14.63 sec\n",
      "Generating proof...\n",
      "Proof gen took 36.56 sec\n",
      "Done with base_model_model_transformer_h_1_attn_c_attn.\n",
      "\n",
      "==========================================\n",
      "Preparing to prove with ONNX: lora_onnx_params/base_model_model_transformer_h_4_attn_c_attn.onnx\n",
      "Matching JSON: intermediate_activations/base_model_model_transformer_h_4_attn_c_attn.json\n",
      "Number of parameters: 24,576\n",
      "Generating settings & compiling circuit...\n",
      "Setup for base_model_model_transformer_h_4_attn_c_attn took 42.12 sec\n",
      "Input shape from JSON: (1, 9216)\n",
      "Local ONNX output shape: (1, 27648)\n",
      "Generating witness (async)...\n",
      "Witness gen took 13.71 sec\n",
      "Generating proof...\n",
      "Proof gen took 35.60 sec\n",
      "Done with base_model_model_transformer_h_4_attn_c_attn.\n",
      "\n",
      "==========================================\n",
      "Preparing to prove with ONNX: lora_onnx_params/base_model_model_transformer_h_0_attn_c_attn.onnx\n",
      "Matching JSON: intermediate_activations/base_model_model_transformer_h_0_attn_c_attn.json\n",
      "Number of parameters: 24,576\n",
      "Generating settings & compiling circuit...\n",
      "Setup for base_model_model_transformer_h_0_attn_c_attn took 45.21 sec\n",
      "Input shape from JSON: (1, 9216)\n",
      "Local ONNX output shape: (1, 27648)\n",
      "Generating witness (async)...\n",
      "Witness gen took 16.27 sec\n",
      "Generating proof...\n",
      "Proof gen took 38.96 sec\n",
      "Done with base_model_model_transformer_h_0_attn_c_attn.\n",
      "\n",
      "==========================================\n",
      "Preparing to prove with ONNX: lora_onnx_params/base_model_model_transformer_h_3_attn_c_attn.onnx\n",
      "Matching JSON: intermediate_activations/base_model_model_transformer_h_3_attn_c_attn.json\n",
      "Number of parameters: 24,576\n",
      "Generating settings & compiling circuit...\n",
      "Setup for base_model_model_transformer_h_3_attn_c_attn took 42.35 sec\n",
      "Input shape from JSON: (1, 9216)\n",
      "Local ONNX output shape: (1, 27648)\n",
      "Generating witness (async)...\n",
      "Witness gen took 15.55 sec\n",
      "Generating proof...\n",
      "Proof gen took 39.88 sec\n",
      "Done with base_model_model_transformer_h_3_attn_c_attn.\n",
      "\n",
      "==========================================\n",
      "Preparing to prove with ONNX: lora_onnx_params/base_model_model_transformer_h_2_attn_c_attn.onnx\n",
      "Matching JSON: intermediate_activations/base_model_model_transformer_h_2_attn_c_attn.json\n",
      "Number of parameters: 24,576\n",
      "Generating settings & compiling circuit...\n",
      "Setup for base_model_model_transformer_h_2_attn_c_attn took 44.40 sec\n",
      "Input shape from JSON: (1, 9216)\n",
      "Local ONNX output shape: (1, 27648)\n",
      "Generating witness (async)...\n",
      "Witness gen took 15.84 sec\n",
      "Generating proof...\n",
      "Proof gen took 38.29 sec\n",
      "Done with base_model_model_transformer_h_2_attn_c_attn.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(297.28650307655334, 88.75116658210754, 227.826753616333, np.int64(147456), 6)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "async def main():\n",
    "    return await generate_proofs(verbose=True)\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying proof for base_model_model_transformer_h_1_attn_c_attn...\n",
      "Verification took 0.50 seconds\n",
      "Proof verified successfully for base_model_model_transformer_h_1_attn_c_attn!\n",
      "\n",
      "Verifying proof for base_model_model_transformer_h_0_attn_c_attn...\n",
      "Verification took 0.47 seconds\n",
      "Proof verified successfully for base_model_model_transformer_h_0_attn_c_attn!\n",
      "\n",
      "Verifying proof for base_model_model_transformer_h_2_attn_c_attn...\n",
      "Verification took 0.45 seconds\n",
      "Proof verified successfully for base_model_model_transformer_h_2_attn_c_attn!\n",
      "\n",
      "Verifying proof for base_model_model_transformer_h_5_attn_c_attn...\n",
      "Verification took 0.47 seconds\n",
      "Proof verified successfully for base_model_model_transformer_h_5_attn_c_attn!\n",
      "\n",
      "Verifying proof for base_model_model_transformer_h_4_attn_c_attn...\n",
      "Verification took 0.46 seconds\n",
      "Proof verified successfully for base_model_model_transformer_h_4_attn_c_attn!\n",
      "\n",
      "Verifying proof for base_model_model_transformer_h_3_attn_c_attn...\n",
      "Verification took 0.46 seconds\n",
      "Proof verified successfully for base_model_model_transformer_h_3_attn_c_attn!\n",
      "\n",
      "Total proofs verified: 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.80391001701355, 6)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_verify_proofs(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
