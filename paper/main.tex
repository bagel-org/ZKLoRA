\documentclass[11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{tikz}
\usetikzlibrary{arrows.meta, positioning, calc, fit}

\title{\textbf{zklora: Efficient Verification of LoRA-Base Model Compatibility for Large Language Models}}
\author{Anonymous Authors}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Verifying that a private Low-Rank Adaptation (LoRA) module is truly compatible with a given large language model (LLM) can be daunting, especially when those LoRA parameters cannot be disclosed. 
We introduce \texttt{zklora}, a library for generating zero-knowledge proofs that demonstrate correct LoRA-base model compatibility \emph{without sharing LoRA weights}. 
A key insight is that the \emph{verification step} for each LoRA module remains on the order of \textbf{1--2 seconds}, enabling repeated or batch checking at scale. 
Our empirical evaluations confirm that \texttt{zklora} maintains this rapid per-module verification time even for multi-billion-parameter LLMs.
\end{abstract}

%------------------------------------------
% INTRO: emphasize verification challenge & clarify it's per-module
%------------------------------------------
\section{Introduction}
Large Language Models (LLMs) have attained remarkable success \cite{brown2020language, devlin2018bert}, but verifying fine-tuned modifications such as LoRA \cite{hu2021lora} can be difficult when the updated weights must remain private. 
Traditionally, one might re-run an entire forward pass or inspect thousands of parameters to ensure correctness, which is infeasible for massive models. 
\texttt{zklora} addresses this by generating a zero-knowledge proof of correctness for each LoRA module, guaranteeing that the private LoRA genuinely fits the public base model. 
Crucially, \textbf{the verification stage for each LoRA module} in \texttt{zklora} remains about 1--2 seconds, even at scales of multi-billion parameter base models. 
This rapid, per-module check allows the Base Model User to efficiently, independently verify the compatibility of the private LoRA parameters.

%------------------------------------------
% 1. PRELIMINARY RESULTS
%------------------------------------------
\section{Preliminary Results}

We benchmarked \texttt{zklora} across several LLMs and smaller models with varying LoRA parameter counts. Our focus is verifying that, regardless of total parameter scale, \emph{the per-module verification step} remains swift. 
Figures~\ref{fig:verify}, \ref{fig:settings}, and \ref{fig:proof} illustrate how total LoRA parameters correlate with:
\begin{itemize}
    \item \textbf{Verification Time per Module} (Base Model User’s overhead),
    \item \textbf{Total Settings Time} (preparatory steps and key generation),
    \item \textbf{Total Proof Generation Time} (LoRA Owner’s overhead).
\end{itemize}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.58\linewidth]{figs/fig_verify.pdf}
    \caption{LoRA params (millions) vs. Total verification. Even at higher scales, verifying each LoRA module remains about 1--2 seconds. For example, even for a 70-billion parameter model, the total verification time of the additional LoRA parameters is only ~2 minutes.}
    \label{fig:verify}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.58\linewidth]{figs/fig_settings.pdf}
    \caption{LoRA params (millions) vs. total settings time. Each point corresponds to a different LLM or smaller model.}
    \label{fig:settings}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.58\linewidth]{figs/fig_proof.pdf}
    \caption{LoRA params (millions) vs. total proof generation time (LoRA Owner’s overhead).}
    \label{fig:proof}
\end{figure}

Although proof generation may scale with parameter size, verification for each LoRA module is consistently on the order of seconds. We consider this \emph{fast, per-module verification} to be \texttt{zklora}’s core advantage in production LLM environments.

%------------------------------------------
% 2. Ensuring LoRA–Base Model Compatibility with zklora
%------------------------------------------
\section{Ensuring LoRA–Base Model Compatibility}

A typical workflow involves:
\begin{enumerate}
    \item \textbf{Base Model User} possessing a large LLM. 
    \item \textbf{LoRA Owner} having proprietary low-rank adapter weights. 
\end{enumerate}
However, these two parties must confirm the LoRA truly “fits” the base model. Using \texttt{zklora}, the LoRA Owner proves the adapter’s compatibility \emph{without} exposing private weights:

\begin{enumerate}
    \item \textbf{Partial Forward Pass.}
    The Base Model User runs the unaltered layers of the LLM, producing partial activations.

    \item \textbf{LoRA Transform + Proof Gen.}
    The LoRA Owner applies private LoRA updates, constructing a zero-knowledge proof to attest correct usage.

    \item \textbf{Verification per LoRA Module (1--2 sec).}
    The user verifies each proof quickly, ensuring the final outputs reflect a valid LoRA for the base model.

    \item \textbf{Evaluate.}
    With trust in the LoRA’s correctness, the user measures performance (e.g., loss).
\end{enumerate}

\begin{figure}[ht]
\centering
\begin{tikzpicture}[
    font=\small,
    >=latex,
    thick,
    block/.style={
      draw,
      rounded corners,
      align=center,
      minimum width=1.8cm,
      minimum height=0.9cm
    },
    dashedblock/.style={
      draw,
      dashed,
      rounded corners,
      align=center,
      font=\footnotesize,
      fill=white,
      minimum width=1.8cm,
      minimum height=0.9cm
    },
    arrow/.style={->, thick},
    node distance=2.0cm
]

\node[block, label=above:\textbf{Input}] (input) {$\mathbf{x}$};
\node[block, right=2.0cm of input] (W1) {$\mathbf{W}_1$};
\node[dashedblock, above=1.6cm of W1, label=above:\textbf{LoRA Owner}] (LoRA) {LoRA for Layer 1};
\node[block, right=2.0cm of W1] (act) {ReLU};
\node[block, right=2.0cm of act] (W2) {$\mathbf{W}_2$};
\node[block, label=above:\textbf{Output}, right=2.0cm of W2] (output) {$\mathbf{y}$};

\draw[->] (input) -- node[above, font=\scriptsize]{local multiply} (W1);
\draw[->, dashed] (W1.north) to[bend left=15] node[left, font=\scriptsize]{partial acts} (LoRA.south);
\draw[->, dashed] (LoRA.south) to[bend left=15] node[right, font=\scriptsize]{updated $\mathbf{z}'$} (W1.north);
\draw[->] (W1) -- node[above, font=\scriptsize]{$\mathbf{z}'$} (act);
\draw[->] (act) -- node[above, font=\scriptsize]{hidden} (W2);
\draw[->] (W2) -- (output);

\node[draw, rounded corners, thick, color=blue!40!black,
      fit=(input)(W1)(act)(W2)(output),
      label={[font=\small, color=blue!40!black]above:\textbf{Base Model User}},
      inner sep=0.4cm
] (baseModelFit) {};
\end{tikzpicture}
\caption{zklora inference flow on a 2-layer neural network where the first layer has private LoRA weights added.}
\label{fig:toy-mlp-lora}
\end{figure}

%------------------------------------------
% 3. HOW WE GENERATE THE ZERO-KNOWLEDGE PROOF
%------------------------------------------
\section{Zero-Knowledge Proof Generation}

\texttt{zklora} compiles an ONNX model (with LoRA included) into a constraint circuit. The user’s partial activations feed the circuit as inputs:

\paragraph{(1) Circuit Compilation.} 
We parse the LoRA-augmented layers, producing a cryptographic circuit capturing each parameter operation. 

\paragraph{(2) Key Setup.} 
A settings file, proving key, and verification key are generated. A structured reference string (SRS) may also be needed.

\paragraph{(3) Witness Creation.}
The LoRA Owner runs the partial activations through the circuit, tracking all wire values in a “witness.” 

\paragraph{(4) Proof.}
Using the witness plus proving key, the LoRA Owner constructs a proof. 

\paragraph{(5) Verification per Module.}
Each LoRA module’s proof is tested with the verification key, taking \textbf{1--2 seconds}. This minimal overhead is \texttt{zklora}’s main advantage for frequent or large-scale checks.

%------------------------------------------
% 4. RELATED WORK
%------------------------------------------
\section{Related Work}

\subsection{Low-Rank Adaptation (LoRA)}

Low-Rank Adaptation (LoRA) \cite{hu2021lora} is a technique for parameter-efficient fine-tuning of large language models (LLMs) that injects small, low-rank adapter matrices into specific layers of a pre-trained model. By isolating the fine-tuning process to these low-rank components, LoRA drastically reduces memory overhead compared to full-model fine-tuning. This design choice is especially appealing for massive LLMs where training or even storing all parameters can be prohibitive \cite{ding2022delta}.


\subsection{Incrementally Verifiable Computation}
In a decentralized world, trust is a resource that is hard to achieve. In decentralized computation, we need to make sure the computation are being done, and are being correctly. In a seminal paper by Valiant (2008) \cite{valiant2008incrementally}, it was shown that proofs of knowledge can be used to assert the correct execution of general computations. That is, if $M$ is a machine that runs for $t$ steps producing a sequence of configurations $c_0,c_1,\dots,c_t$, then there exist an efficient and effective way to produce a computationally sound proof for the computation $c_0  \xrightarrow{t} c_t$. This idea is referred to as Incrementally Verifiable Computation or IVC.

The main goal is IVC is to produce compact, updatable proofs of correctness for a sequence of computations, so that each new step in the computation can be verified on its own while building on the guarantees of the previous steps. This technique significantly reduces the verification overhead for long or evolving computations, which is invaluable in scenarios like decentralized networks, outsourced computation, and any application requiring frequent correctness checks.

Kumar et al. (2021) \cite{kothapalli2022nova} introduced the proof system NOVA and the idea of recursive proofs, which are proofs that can ``prove the correctness of other proofs.'' Recursive proof composition is key to IVC where each proof attests to the correctness of both a step’s output and the validity of the previous step’s proof.

HyperNova \cite{kothapalli2024hypernova} is a novel recursive argument system optimized for customizable constraint systems (CCS) that generalizes and improves upon prior approaches like Nova. It achieves efficiency through a folding scheme where the prover’s cryptographic costs are minimized and achieves zero-knowledge without relying on zkSNARKs.

An IVC system allows the construction of proofs in zero-knowledge where the   proofs reveal no information about the underlying computation or its inputs beyond the validity of the claim \cite{valiant2008incrementally}.


%------------------------------------------
% 5. CONCLUSION
%------------------------------------------
\section{Conclusion}

\texttt{zklora} provides a fast, robust mechanism to ensure that private LoRA modules remain consistent with a large base model. Our evaluations indicate that each LoRA module’s correctness can be verified in about 1--2 seconds, even for multi-billion-parameter LLMs. This efficiency bridges the gap between privacy-preserving LoRA development and practical, real-time validation in large-scale deployments. Future expansions could integrate multi-owner LoRAs, advanced zero-knowledge proofs for further performance gains, or partial data-privacy frameworks to shield user inputs as well as LoRA parameters.

\bibliographystyle{plain}
\bibliography{references}

\end{document}
